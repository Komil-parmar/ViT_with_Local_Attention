
# **Vision Transformer with Local Attention ğŸš€**

ğŸ” **A self-implementation of Vision Transformer (ViT) with Local Attention Mechanism.**  

## **ğŸŒŸ Features**
âœ… Patch Embedding with Learnable Tokens  
âœ… Multi-Head **Global** and **Local** Self-Attention  
âœ… Mix **Global** and **Local** Self-Attention Mechanism within the same model  
âœ… Add **CLS** Token to the local attention window of each patch
âœ… Transformer Encoder with Configurable Layers  
âœ… Customizable Vision Transformer for Various Image Sizes  
âœ… Designed for GPU Acceleration ğŸš€  

## **ğŸ“ Note**
â— **Important:** Using Local Attention Mechanism in Vision Transformer makes the computations much faster and efficient but does not reduce the complexity (number of parameters) of the model.

## **ğŸ“‚ Repository Structure**
```
ğŸ“ ViT
 â”œâ”€â”€ ViT.py               # Main implementation of ViT with Local Attention
 â”œâ”€â”€ README.md            # You are here! ğŸ“œ
 â”œâ”€â”€ examples/            # Example scripts (Coming Soon)  
 â”œâ”€â”€ experiments/         # Training and testing experiments (Coming Soon)  
```

## **âš¡ Quick Start**
```python
from ViT import VisionTransformer

model = VisionTransformer(
    img_size=32, patch_size=4, in_channels=3, num_classes=10,
    embed_dim=128, num_heads=4, num_layers=6, mlp_dim=256,
    dropout=0.1, window_size=[3, 3, 5, 5, None, None], add_cls_token=True
)
print(model)
```

## **ğŸ“œ License & Usage Notice**
â— **Important:** This repository is currently **not licensed**.  
If you use any part of this code, please **provide credit** and notify me via [GitHub Issues](https://github.com/komil-parmar/ViT_with_Local_Attention/issues) or email.  
ğŸ’¬ Friendly Note: If you're just experimenting, learning, or using this code for research, no need to add any formalities while contacting me!  
Feel free to explore and have fun with it. 
A formal open-source license will be added later.  

## **ğŸš€ Future Work**
- ğŸ“Œ Add training and fine-tuning scripts  
- ğŸ“Œ Implement efficient local attention techniques  
- ğŸ“Œ Implement variable sized cls token (n patches)
- ğŸ“Œ Release benchmark results  

## **ğŸ’¬ Contributing**
ğŸ’¡ Suggestions and improvements are welcome! Feel free to open an issue or contribute via pull requests.  

## **ğŸ‘¨â€ğŸ“ About Me**
Hi! I'm a college student, self-learning deep learning and AI.
I truly appreciate any kind of feedback, reviews, or suggestions, as they help me grow! ğŸ˜Š

ğŸ“Œ You can learn more about me on my GitHub: [GitHub Profile](https://github.com/Komil-parmar/Komil-parmar)
